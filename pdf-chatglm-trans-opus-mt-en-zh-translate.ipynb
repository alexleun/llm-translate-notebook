{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401a6884-8cb1-42c8-9ec4-3194a40f4ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf\n",
    "# PDF file, using ChatGLM (GPU) + trans-opus-mt-en-zh (CPU)\n",
    "# trans-opus-mt-en-zh is use for translate short content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0232bd74-867f-444c-9c2e-823b0f6ed2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16859613-e8c5-49c0-a54c-b8bb0f8bfbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"Your_file_here.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f59441-7e3a-4943-9aa3-5bbd8b3c21bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(pages[0].dict()['page_content'])\n",
    "l={}\n",
    "c=0\n",
    "print(f\"{len(pages)}\")\n",
    "for x in pages:\n",
    "    l[c]=len(x.dict()['page_content'])\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a756502-4037-44e0-a66e-c9320fd67381",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2=sorted(l.values())\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2a767-7f47-43cf-8a6e-d602e0c5e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(l, key=l.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d20bb-5a12-4fd1-b3ad-2d61f146e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[max(l, key=l.get)].dict()['page_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1188c7-0a61-45d1-90e0-d91692428fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages[17].dict()['page_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d635c77c-953c-4a27-836d-7e370bb73208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"\n",
    "翻译成中文:\\n\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb015c61-e55a-4334-aff3-f970cca5bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load translate module\n",
    "from transformers import AutoModelWithLMHead,AutoTokenizer,pipeline\n",
    "mode_name = 'liam168/trans-opus-mt-en-zh'\n",
    "model = AutoModelWithLMHead.from_pretrained(mode_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_name)\n",
    "translation = pipeline(\"translation_en_to_zh\", model=model, tokenizer=tokenizer, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fb2940-3845-4816-af7d-4e3bc74d82c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e72cfc-7a20-4416-bbce-3556df7d4a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996cff4-0183-408a-9636-39fdbfdae900",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = IntProgress(max=len(pages))\n",
    "p2.description = 'Running'\n",
    "display(p2)\n",
    "b=''\n",
    "x=-1\n",
    "# translated to 102\n",
    "with open(r'Your_result_here.txt', 'a', encoding='UTF-8') as fp:\n",
    "    for i in pages:\n",
    "        x+=1\n",
    "        p2.value+=1\n",
    "        # Use debug mode to monitor \"x\" if you what to continue translation next time.\n",
    "        # Replace '0' to pervious last status value. \n",
    "        if x > 0:\n",
    "            max_len=len(i.dict()['page_content'])\n",
    "            res = i.dict()['page_content']\n",
    "            # Translate if text len is longer then 100\n",
    "            if max_len > 100:\n",
    "                # Use CPU for text len smaller then 900\n",
    "                if max_len < 900:\n",
    "                    temp=translation(i.dict()['page_content'], max_length=max_len)\n",
    "                    res= temp[0]['translation_text']\n",
    "                else:\n",
    "                    res, history = model.chat(tokenizer, prompt.format(question=i.dict()['page_content']), history=[])\n",
    "            b= b + res + '\\n'\n",
    "            fp.write('\\n'+ res)\n",
    "            fp.flush()\n",
    "    fp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
