{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c187f6c-07a2-43db-9446-d2bd9082eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Txt file, using ChatGLM with cude support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813083d6-0152-45ef-8df5-7206ac4d6d1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "# Load file and chunk large file into a list object\n",
    "# if your file is japan, use this\n",
    "# f = io.open(\"Your_source_file_here.txt\", mode=\"r\", encoding=\"utf-8\")\n",
    "# if your file is English, use this\n",
    "f = io.open(\"Your_source_file_here.txt\", mode=\"r\")\n",
    "s = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd0925-1c1e-42eb-9df4-c15de0e6e286",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"\n",
    "翻译成中文:\\n\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b1e101-2ef0-4892-a632-9cfd9487b36b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "text_splitter2 = SpacyTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter2.split_text(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be751143-fbfd-4737-b898-96b367c99842",
   "metadata": {},
   "outputs": [],
   "source": [
    "del text_splitter2, s, f, io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848735a-bad6-44ef-86c1-83b6fc24d16f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b4a01-b5d3-45e3-b3d1-2c057fdc89f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "result=[]\n",
    "p2 = IntProgress(max=len(texts))\n",
    "p2.description = 'Running'\n",
    "display(p2)\n",
    "x = -1\n",
    "with open(r'Your_Translated_temp_result.txt', 'a', encoding='UTF-8') as fp:\n",
    "    for i in texts:\n",
    "        p2.value += 1\n",
    "        x += 1\n",
    "        # Use debug mode to monitor \"x\" if you what to continue translation next time.\n",
    "        # Replace '0' to pervious last status value. \n",
    "        if x > 0:\n",
    "            response, history = model.chat(tokenizer, prompt.format(question=i), history=[])\n",
    "            # print(response)\n",
    "            fp.write('\\n'+ response)\n",
    "            fp.flush()\n",
    "    fp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
